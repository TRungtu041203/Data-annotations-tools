{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b771203b",
   "metadata": {},
   "source": [
    "# YOLO Dataset Preparation and Training Notebook\n",
    "\n",
    "This notebook provides a comprehensive workflow for:\n",
    "1. **Dataset Preparation**: Prepare two types of YOLO datasets - one with background fusion and one with original images\n",
    "2. **Data Conversion**: Convert JSON annotations to YOLO format\n",
    "3. **Model Training**: Fine-tune YOLOv8 models on both datasets\n",
    "4. **Comparison**: Compare performance between original and background-fused datasets\n",
    "\n",
    "## Prerequisites\n",
    "- Ultralytics YOLOv8 installed (`pip install ultralytics`)\n",
    "- Your data directory containing images and JSON annotation files\n",
    "- Background-fused images (ending with `_fused.jpg`) if using background fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc3272e",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b1cd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ultralytics YOLO imported successfully\n",
      "âœ… PyTorch version: 2.5.1+cu121\n",
      "âœ… CUDA available: True\n",
      "âœ… GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "âœ… All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if ultralytics is available\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    import torch\n",
    "    print(\"âœ… Ultralytics YOLO imported successfully\")\n",
    "    print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "    print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"âœ… GPU: {torch.cuda.get_device_name()}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Ultralytics not found. Install with: pip install ultralytics\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb05bf0",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup\n",
    "\n",
    "Configure paths and parameters for dataset preparation and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dbe5c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Configuration:\n",
      "   Source Directory: out_pro_plus\n",
      "   Output Directory: datasets\n",
      "   Pretrained Model: yolo_model\\yolo12x.pt\n",
      "   Train/Val Split: 80.0%/20.0%\n",
      "   Training Epochs: 100\n",
      "   Batch Size: 8\n",
      "   Image Size: 1024\n",
      "\n",
      "ğŸ“Š Source Directory Analysis:\n",
      "   Original images: 7202\n",
      "   Fused images: 7121\n",
      "   JSON files: 7121\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# =========================\n",
    "\n",
    "# Paths\n",
    "SOURCE_DIR = Path(\"out_pro_plus\")  # Directory with images and JSON files\n",
    "OUTPUT_BASE_DIR = Path(\"datasets\")    # Base directory for prepared datasets\n",
    "PRETRAINED_MODEL_PATH = Path(\"yolo_model/yolo12x.pt\")  \n",
    "\n",
    "# Dataset parameters\n",
    "TRAIN_RATIO = 0.8  # 80% for training, 20% for validation\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Training parameters\n",
    "TRAINING_CONFIG = {\n",
    "    'epochs': 100,\n",
    "    'batch_size': 8,\n",
    "    'image_size': 1024,\n",
    "    'learning_rate': 0.001,\n",
    "    'patience': 20,\n",
    "    'save_period': 10,\n",
    "    'workers': 0,  # Use 0 on Windows to avoid DataLoader issues\n",
    "}\n",
    "\n",
    "# Display configuration\n",
    "print(\"ğŸ“ Configuration:\")\n",
    "print(f\"   Source Directory: {SOURCE_DIR}\")\n",
    "print(f\"   Output Directory: {OUTPUT_BASE_DIR}\")\n",
    "print(f\"   Pretrained Model: {PRETRAINED_MODEL_PATH}\")\n",
    "print(f\"   Train/Val Split: {TRAIN_RATIO:.1%}/{1-TRAIN_RATIO:.1%}\")\n",
    "print(f\"   Training Epochs: {TRAINING_CONFIG['epochs']}\")\n",
    "print(f\"   Batch Size: {TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"   Image Size: {TRAINING_CONFIG['image_size']}\")\n",
    "\n",
    "# Check if source directory exists\n",
    "if SOURCE_DIR.exists():\n",
    "    jpg_files = list(SOURCE_DIR.glob(\"*.jpg\"))\n",
    "    fused_files = list(SOURCE_DIR.glob(\"*_fused.jpg\"))\n",
    "    json_files = list(SOURCE_DIR.glob(\"*.json\"))\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Source Directory Analysis:\")\n",
    "    print(f\"   Original images: {len(jpg_files) - len(fused_files)}\")\n",
    "    print(f\"   Fused images: {len(fused_files)}\")\n",
    "    print(f\"   JSON files: {len(json_files)}\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Source directory {SOURCE_DIR} does not exist!\")\n",
    "    print(\"   Please update SOURCE_DIR variable or create the directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3daab8",
   "metadata": {},
   "source": [
    "## 3. YOLO Dataset Preparation Class\n",
    "\n",
    "This class combines all the functionality from the three original files:\n",
    "- `prep_dataset.py` - Dataset organization and splitting\n",
    "- `json_to_yolo.py` - JSON to YOLO format conversion\n",
    "- `clean_json.py` - JSON file cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a3ef2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… YOLODatasetPreparator initialized!\n"
     ]
    }
   ],
   "source": [
    "class YOLODatasetPreparator:\n",
    "    \"\"\"\n",
    "    Comprehensive YOLO dataset preparation class.\n",
    "    Combines functionality from prep_dataset.py, json_to_yolo.py, and clean_json.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seed: int = 42):\n",
    "        \"\"\"Initialize the dataset preparator with random seed.\"\"\"\n",
    "        random.seed(seed)\n",
    "        self.seed = seed\n",
    "    \n",
    "    def find_images_and_labels(self, source_dir: Path, use_fused: bool = False) -> List[Tuple[Path, Optional[Path]]]:\n",
    "        \"\"\"Find image-label pairs in the source directory.\"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        if use_fused:\n",
    "            # Look for fused images and corresponding JSON files\n",
    "            fused_images = list(source_dir.glob(\"*_fused.jpg\"))\n",
    "            print(f\"Found {len(fused_images)} fused images in {source_dir}\")\n",
    "            \n",
    "            for image_path in fused_images:\n",
    "                # Remove _fused suffix to find corresponding JSON\n",
    "                json_name = image_path.stem.replace(\"_fused\", \"\") + \".json\"\n",
    "                json_path = source_dir / json_name\n",
    "                \n",
    "                if json_path.exists():\n",
    "                    pairs.append((image_path, json_path))\n",
    "                else:\n",
    "                    # Include images without JSON (no detections = negative examples)\n",
    "                    pairs.append((image_path, None))\n",
    "            \n",
    "            # Add original images that don't have JSON files as negative examples\n",
    "            # This ensures fused dataset has same negative examples as original dataset\n",
    "            original_images = [img for img in source_dir.glob(\"*.jpg\") if not img.name.endswith(\"_fused.jpg\")]\n",
    "            negative_originals = []\n",
    "            \n",
    "            for image_path in original_images:\n",
    "                json_path = source_dir / f\"{image_path.stem}.json\"\n",
    "                if not json_path.exists():\n",
    "                    # This original image has no detections, add it as negative example\n",
    "                    pairs.append((image_path, None))\n",
    "                    negative_originals.append(image_path)\n",
    "            \n",
    "            if negative_originals:\n",
    "                print(f\"Added {len(negative_originals)} original images without detections as negative examples\")\n",
    "                    \n",
    "        else:\n",
    "            # Look for original images and corresponding JSON files\n",
    "            jpg_images = [img for img in source_dir.glob(\"*.jpg\") if not img.name.endswith(\"_fused.jpg\")]\n",
    "            print(f\"Found {len(jpg_images)} original images in {source_dir}\")\n",
    "            \n",
    "            for image_path in jpg_images:\n",
    "                json_path = source_dir / f\"{image_path.stem}.json\"\n",
    "                \n",
    "                if json_path.exists():\n",
    "                    pairs.append((image_path, json_path))\n",
    "                else:\n",
    "                    # Include images without JSON (no detections = negative examples)\n",
    "                    pairs.append((image_path, None))\n",
    "        \n",
    "        # Count positive vs negative examples\n",
    "        positive_pairs = sum(1 for _, json_path in pairs if json_path is not None)\n",
    "        negative_pairs = len(pairs) - positive_pairs\n",
    "        \n",
    "        print(f\"Found {len(pairs)} total images:\")\n",
    "        print(f\"  â€¢ {positive_pairs} images with detections (positive examples)\")\n",
    "        print(f\"  â€¢ {negative_pairs} images without detections (negative examples)\")\n",
    "        return pairs\n",
    "    \n",
    "    def split_train_val(self, pairs: List[Tuple[Path, Optional[Path]]], train_ratio: float = 0.8) -> Tuple[List[Tuple[Path, Optional[Path]]], List[Tuple[Path, Optional[Path]]]]:\n",
    "        \"\"\"Split image-label pairs into train and validation sets.\"\"\"\n",
    "        shuffled_pairs = pairs.copy()\n",
    "        random.shuffle(shuffled_pairs)\n",
    "        \n",
    "        train_count = int(len(shuffled_pairs) * train_ratio)\n",
    "        train_pairs = shuffled_pairs[:train_count]\n",
    "        val_pairs = shuffled_pairs[train_count:]\n",
    "        \n",
    "        print(f\"Split: {len(train_pairs)} train, {len(val_pairs)} validation\")\n",
    "        return train_pairs, val_pairs\n",
    "    \n",
    "    def setup_yolo_directories(self, yolo_dir: Path):\n",
    "        \"\"\"Create YOLO dataset directory structure.\"\"\"\n",
    "        dirs_to_create = [\n",
    "            yolo_dir / \"images\" / \"train\",\n",
    "            yolo_dir / \"images\" / \"val\", \n",
    "            yolo_dir / \"labels\" / \"train\",\n",
    "            yolo_dir / \"labels\" / \"val\"\n",
    "        ]\n",
    "        \n",
    "        for dir_path in dirs_to_create:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "        print(f\"âœ… Created YOLO directory structure in {yolo_dir}\")\n",
    "    \n",
    "    def convert_json_to_yolo(self, json_path: Optional[Path], output_file: Path) -> int:\n",
    "        \"\"\"Convert single JSON file to YOLO format, or create empty file if no JSON.\"\"\"\n",
    "        if json_path is None or not json_path.exists():\n",
    "            # Create empty label file for images with no detections\n",
    "            with open(output_file, 'w') as f:\n",
    "                f.write(\"\")  # Empty file\n",
    "            return 0\n",
    "        \n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        img_width = data['img_width']\n",
    "        img_height = data['img_height']\n",
    "        \n",
    "        yolo_lines = []\n",
    "        for annotation in data['annotations']:\n",
    "            x1, y1, x2, y2 = annotation['box']\n",
    "            \n",
    "            # Convert to YOLO format (normalized xywh)\n",
    "            x_center = (x1 + x2) / 2.0 / img_width\n",
    "            y_center = (y1 + y2) / 2.0 / img_height\n",
    "            width = (x2 - x1) / img_width\n",
    "            height = (y2 - y1) / img_height\n",
    "            \n",
    "            # Class ID (0 for person)\n",
    "            class_id = 0\n",
    "            \n",
    "            yolo_line = f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
    "            yolo_lines.append(yolo_line)\n",
    "        \n",
    "        # Write YOLO format file\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write('\\n'.join(yolo_lines))\n",
    "        \n",
    "        return len(yolo_lines)\n",
    "    \n",
    "    def copy_and_convert_pairs(self, pairs: List[Tuple[Path, Optional[Path]]], yolo_dir: Path, split_name: str):\n",
    "        \"\"\"Copy images and convert JSON labels to YOLO format.\"\"\"\n",
    "        images_dir = yolo_dir / \"images\" / split_name\n",
    "        labels_dir = yolo_dir / \"labels\" / split_name\n",
    "        \n",
    "        copied_images = 0\n",
    "        total_annotations = 0\n",
    "        empty_labels = 0\n",
    "        \n",
    "        for image_path, json_path in pairs:\n",
    "            # Copy image\n",
    "            dest_image = images_dir / image_path.name\n",
    "            shutil.copy2(image_path, dest_image)\n",
    "            copied_images += 1\n",
    "            \n",
    "            # Convert JSON to YOLO format (or create empty label file)\n",
    "            label_name = image_path.stem + \".txt\"\n",
    "            dest_label = labels_dir / label_name\n",
    "            annotations_count = self.convert_json_to_yolo(json_path, dest_label)\n",
    "            \n",
    "            if annotations_count == 0:\n",
    "                empty_labels += 1\n",
    "            total_annotations += annotations_count\n",
    "        \n",
    "        print(f\"âœ… Copied {copied_images} images and converted {total_annotations} annotations to {split_name} set\")\n",
    "        if empty_labels > 0:\n",
    "            print(f\"   ğŸ“ Created {empty_labels} empty label files for images without detections (negative examples)\")\n",
    "        return copied_images, total_annotations\n",
    "    \n",
    "    def create_yaml_config(self, yolo_dir: Path, dataset_name: str):\n",
    "        \"\"\"Create YOLO dataset configuration YAML file.\"\"\"\n",
    "        # Use absolute path to avoid path resolution issues\n",
    "        absolute_path = yolo_dir.resolve()\n",
    "        \n",
    "        yaml_content = f\"\"\"# YOLO dataset configuration for {dataset_name}\n",
    "path: {absolute_path}\n",
    "train: images/train\n",
    "val: images/val\n",
    "\n",
    "names:\n",
    "  0: person\n",
    "\n",
    "# Dataset info\n",
    "nc: 1  # number of classes\n",
    "\"\"\"\n",
    "        \n",
    "        yaml_file = yolo_dir / f\"{yolo_dir.name}.yaml\"\n",
    "        with open(yaml_file, 'w') as f:\n",
    "            f.write(yaml_content)\n",
    "        \n",
    "        print(f\"âœ… Created YAML config: {yaml_file}\")\n",
    "        print(f\"   ğŸ“ Dataset path: {absolute_path}\")\n",
    "        return yaml_file\n",
    "    \n",
    "    def prepare_dataset(self, source_dir: Path, output_dir: Path, dataset_name: str, \n",
    "                       use_fused: bool = False, train_ratio: float = 0.8):\n",
    "        \"\"\"Prepare complete YOLO dataset.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸš€ Preparing {dataset_name} dataset\")\n",
    "        print(f\"ğŸ“ Source: {source_dir}\")\n",
    "        print(f\"ğŸ“ Output: {output_dir}\")\n",
    "        print(f\"ğŸ–¼ï¸  Using {'fused' if use_fused else 'original'} images\")\n",
    "        print(f\"ğŸ“Š Train/Val split: {train_ratio:.1%}/{1-train_ratio:.1%}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Find image-label pairs\n",
    "        pairs = self.find_images_and_labels(source_dir, use_fused)\n",
    "        if not pairs:\n",
    "            print(f\"âŒ No valid image-label pairs found in {source_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Split into train/val\n",
    "        train_pairs, val_pairs = self.split_train_val(pairs, train_ratio)\n",
    "        \n",
    "        # Create directory structure\n",
    "        self.setup_yolo_directories(output_dir)\n",
    "        \n",
    "        # Copy and convert files\n",
    "        train_images, train_annotations = self.copy_and_convert_pairs(train_pairs, output_dir, \"train\")\n",
    "        val_images, val_annotations = self.copy_and_convert_pairs(val_pairs, output_dir, \"val\")\n",
    "        \n",
    "        # Create YAML configuration\n",
    "        yaml_file = self.create_yaml_config(output_dir, dataset_name)\n",
    "        \n",
    "        print(f\"\\nâœ… {dataset_name} dataset prepared!\")\n",
    "        print(f\"   ğŸ“Š Train: {train_images} images, {train_annotations} annotations\")\n",
    "        print(f\"   ğŸ“Š Val: {val_images} images, {val_annotations} annotations\")\n",
    "        print(f\"   ğŸ“„ Config: {yaml_file}\")\n",
    "        \n",
    "        return {\n",
    "            'train_images': train_images,\n",
    "            'train_annotations': train_annotations,\n",
    "            'val_images': val_images,\n",
    "            'val_annotations': val_annotations,\n",
    "            'yaml_file': yaml_file,\n",
    "            'output_dir': output_dir\n",
    "        }\n",
    "    \n",
    "    def subsample_dataset(self, source_dir: Path, output_dir: Path, dataset_name: str,\n",
    "                         sample_size: int, use_fused: bool = False, train_ratio: float = 0.8):\n",
    "        \"\"\"Create a subsampled dataset with a specific number of images.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ¯ Preparing {dataset_name} dataset (subsample: {sample_size} images)\")\n",
    "        print(f\"ğŸ“ Source: {source_dir}\")\n",
    "        print(f\"ğŸ“ Output: {output_dir}\")\n",
    "        print(f\"ğŸ–¼ï¸  Using {'fused' if use_fused else 'original'} images\")\n",
    "        print(f\"ğŸ“Š Train/Val split: {train_ratio:.1%}/{1-train_ratio:.1%}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Find all image-label pairs\n",
    "        all_pairs = self.find_images_and_labels(source_dir, use_fused)\n",
    "        if not all_pairs:\n",
    "            print(f\"âŒ No valid image-label pairs found in {source_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Check if we have enough images\n",
    "        if len(all_pairs) < sample_size:\n",
    "            print(f\"âš ï¸  Warning: Only {len(all_pairs)} images available, using all of them\")\n",
    "            sample_size = len(all_pairs)\n",
    "        \n",
    "        # Separate positive and negative examples for balanced subsampling\n",
    "        positive_pairs = [(img, json_path) for img, json_path in all_pairs if json_path is not None]\n",
    "        negative_pairs = [(img, json_path) for img, json_path in all_pairs if json_path is None]\n",
    "        \n",
    "        print(f\"ğŸ“Š Available data: {len(positive_pairs)} positive, {len(negative_pairs)} negative examples\")\n",
    "        \n",
    "        # Calculate how many of each type to include (try to maintain balance)\n",
    "        total_available = len(all_pairs)\n",
    "        positive_ratio = len(positive_pairs) / total_available\n",
    "        \n",
    "        target_positive = min(int(sample_size * positive_ratio), len(positive_pairs))\n",
    "        target_negative = min(sample_size - target_positive, len(negative_pairs))\n",
    "        \n",
    "        # If we don't have enough negatives, take more positives\n",
    "        if target_negative < sample_size - target_positive:\n",
    "            target_positive = min(sample_size - target_negative, len(positive_pairs))\n",
    "        \n",
    "        print(f\"ğŸ¯ Subsampling: {target_positive} positive + {target_negative} negative = {target_positive + target_negative} total\")\n",
    "        \n",
    "        # Randomly sample from each category\n",
    "        random.shuffle(positive_pairs)\n",
    "        random.shuffle(negative_pairs)\n",
    "        \n",
    "        subsampled_pairs = positive_pairs[:target_positive] + negative_pairs[:target_negative]\n",
    "        \n",
    "        # Split into train/val\n",
    "        train_pairs, val_pairs = self.split_train_val(subsampled_pairs, train_ratio)\n",
    "        \n",
    "        # Create directory structure\n",
    "        self.setup_yolo_directories(output_dir)\n",
    "        \n",
    "        # Copy and convert files\n",
    "        train_images, train_annotations = self.copy_and_convert_pairs(train_pairs, output_dir, \"train\")\n",
    "        val_images, val_annotations = self.copy_and_convert_pairs(val_pairs, output_dir, \"val\")\n",
    "        \n",
    "        # Create YAML configuration\n",
    "        yaml_file = self.create_yaml_config(output_dir, dataset_name)\n",
    "        \n",
    "        print(f\"\\nâœ… {dataset_name} dataset prepared!\")\n",
    "        print(f\"   ğŸ“Š Train: {train_images} images, {train_annotations} annotations\")\n",
    "        print(f\"   ğŸ“Š Val: {val_images} images, {val_annotations} annotations\")\n",
    "        print(f\"   ğŸ“„ Config: {yaml_file}\")\n",
    "        \n",
    "        return {\n",
    "            'train_images': train_images,\n",
    "            'train_annotations': train_annotations,\n",
    "            'val_images': val_images,\n",
    "            'val_annotations': val_annotations,\n",
    "            'yaml_file': yaml_file,\n",
    "            'output_dir': output_dir,\n",
    "            'sample_size': target_positive + target_negative,\n",
    "            'positive_samples': target_positive,\n",
    "            'negative_samples': target_negative\n",
    "        }\n",
    "\n",
    "# Initialize the dataset preparator\n",
    "preparator = YOLODatasetPreparator(seed=RANDOM_SEED)\n",
    "print(\"âœ… YOLODatasetPreparator initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7254e835",
   "metadata": {},
   "source": [
    "## 4. Prepare Both Datasets\n",
    "\n",
    "Now we'll prepare both datasets:\n",
    "1. **Original Dataset**: Using original images without background fusion\n",
    "2. **Fused Dataset**: Using background-fused images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c4fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare both datasets\n",
    "datasets = {}\n",
    "\n",
    "# 1. Prepare Original Dataset (without background fusion)\n",
    "print(\"ğŸ¯ Step 1: Preparing Original Dataset...\")\n",
    "original_result = preparator.prepare_dataset(\n",
    "    source_dir=SOURCE_DIR,\n",
    "    output_dir=OUTPUT_BASE_DIR / \"yolo_original\",\n",
    "    dataset_name=\"Original Images\",\n",
    "    use_fused=False,\n",
    "    train_ratio=TRAIN_RATIO\n",
    ")\n",
    "\n",
    "if original_result:\n",
    "    datasets['original'] = original_result\n",
    "    print(\"âœ… Original dataset prepared successfully!\")\n",
    "else:\n",
    "    print(\"âŒ Failed to prepare original dataset\")\n",
    "\n",
    "# 2. Prepare Fused Dataset (with background fusion)\n",
    "print(\"\\nğŸ¯ Step 2: Preparing Background-Fused Dataset...\")\n",
    "fused_result = preparator.prepare_dataset(\n",
    "    source_dir=SOURCE_DIR,\n",
    "    output_dir=OUTPUT_BASE_DIR / \"yolo_fused\",\n",
    "    dataset_name=\"Background Fused\",\n",
    "    use_fused=True,\n",
    "    train_ratio=TRAIN_RATIO\n",
    ")\n",
    "\n",
    "if fused_result:\n",
    "    datasets['fused'] = fused_result\n",
    "    print(\"âœ… Fused dataset prepared successfully!\")\n",
    "else:\n",
    "    print(\"âŒ Failed to prepare fused dataset\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“‹ DATASET PREPARATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for dataset_name, result in datasets.items():\n",
    "    if result:\n",
    "        print(f\"\\n{dataset_name.upper()} Dataset:\")\n",
    "        print(f\"  ğŸ“Š Train: {result['train_images']} images, {result['train_annotations']} annotations\")\n",
    "        print(f\"  ğŸ“Š Val: {result['val_images']} images, {result['val_annotations']} annotations\")\n",
    "        print(f\"  ğŸ“„ Config: {result['yaml_file']}\")\n",
    "        print(f\"  ğŸ“ Directory: {result['output_dir']}\")\n",
    "\n",
    "print(f\"ğŸ‰ All datasets ready for YOLO training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d105b026",
   "metadata": {},
   "source": [
    "## 5. Create Subsampled Datasets for Testing\n",
    "\n",
    "For testing and experimentation, we'll create smaller subsampled versions of both datasets.\n",
    "This allows for faster training iterations to test different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890cf193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SUBSAMPLING CONFIGURATION\n",
    "# =========================\n",
    "\n",
    "# Define the subsample sizes you want to test\n",
    "SUBSAMPLE_SIZES = [500, 1000, 2000, 5000]\n",
    "\n",
    "# Only create subsamples if we have enough data\n",
    "def get_available_subsample_sizes(dataset_info):\n",
    "    \"\"\"Determine which subsample sizes are feasible based on available data.\"\"\"\n",
    "    if not dataset_info:\n",
    "        return []\n",
    "    \n",
    "    total_images = dataset_info['train_images'] + dataset_info['val_images']\n",
    "    feasible_sizes = [size for size in SUBSAMPLE_SIZES if size <= total_images]\n",
    "    \n",
    "    if not feasible_sizes and total_images > 0:\n",
    "        # If none of the preset sizes work, use the maximum available\n",
    "        feasible_sizes = [total_images]\n",
    "    \n",
    "    return feasible_sizes\n",
    "\n",
    "# Check feasible sizes for each dataset\n",
    "print(\"ğŸ“Š Checking feasible subsample sizes...\")\n",
    "for dataset_name, dataset_info in datasets.items():\n",
    "    feasible_sizes = get_available_subsample_sizes(dataset_info)\n",
    "    print(f\"{dataset_name.upper()}: {feasible_sizes} (total available: {dataset_info['train_images'] + dataset_info['val_images'] if dataset_info else 0})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d936439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CREATE SUBSAMPLED DATASETS\n",
    "# =========================\n",
    "\n",
    "subsampled_datasets = {}\n",
    "\n",
    "for dataset_type in ['original', 'fused']:\n",
    "    if dataset_type not in datasets or not datasets[dataset_type]:\n",
    "        print(f\"âš ï¸  Skipping {dataset_type} dataset - not available\")\n",
    "        continue\n",
    "\n",
    "    dataset_info = datasets[dataset_type]\n",
    "    feasible_sizes = get_available_subsample_sizes(dataset_info)\n",
    "\n",
    "    if not feasible_sizes:\n",
    "        print(f\"âš ï¸  No feasible subsample sizes for {dataset_type} dataset\")\n",
    "        continue\n",
    "\n",
    "    subsampled_datasets[dataset_type] = {}\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ğŸ¯ Creating subsampled {dataset_type.upper()} datasets\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ğŸ“Š Feasible sizes: {feasible_sizes}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for sample_size in feasible_sizes:\n",
    "        print(\"\\n\" + f\"ğŸ”¸ Creating {sample_size}-image subset...\")\n",
    "\n",
    "        # Create subdirectory for this sample size\n",
    "        subsample_dir = OUTPUT_BASE_DIR / f\"yolo_{dataset_type}_sub{sample_size}\"\n",
    "\n",
    "        # Prepare subsampled dataset\n",
    "        result = preparator.subsample_dataset(\n",
    "            source_dir=SOURCE_DIR,\n",
    "            output_dir=subsample_dir,\n",
    "            dataset_name=f\"{dataset_type.title()} ({sample_size} images)\",\n",
    "            sample_size=sample_size,\n",
    "            use_fused=(dataset_type == 'fused'),\n",
    "            train_ratio=TRAIN_RATIO\n",
    "        )\n",
    "\n",
    "        if result:\n",
    "            subsampled_datasets[dataset_type][sample_size] = result\n",
    "            print(f\"âœ… {sample_size}-image {dataset_type} subset created!\")\n",
    "        else:\n",
    "            print(f\"âŒ Failed to create {sample_size}-image {dataset_type} subset\")\n",
    "\n",
    "# Display summary of all subsampled datasets\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“‹ SUBSAMPLED DATASETS SUMMARY\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "for dataset_type, subsamples in subsampled_datasets.items():\n",
    "    if subsamples:\n",
    "        print(f\"{dataset_type.upper()} Subsamples:\")\n",
    "        for sample_size, result in subsamples.items():\n",
    "            print(f\"  ğŸ“Š {sample_size} images:\")\n",
    "            print(f\"      Train: {result['train_images']} images, {result['train_annotations']} annotations\")\n",
    "            print(f\"      Val:   {result['val_images']} images, {result['val_annotations']} annotations\")\n",
    "            print(f\"      Positive/Negative: {result['positive_samples']}/{result['negative_samples']}\")\n",
    "            print(f\"      Config: {result['yaml_file']}\")\n",
    "        print()\n",
    "\n",
    "total_subsamples = sum(len(v) for v in subsampled_datasets.values())\n",
    "print(f\"\\nğŸ‰ Created {total_subsamples} subsampled datasets ready for testing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d099066e",
   "metadata": {},
   "source": [
    "## 5. YOLO Training Class\n",
    "\n",
    "This class handles model training, evaluation, and comparison between datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e9ef73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… YOLOTrainer initialized!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class YOLOTrainer:\n",
    "    \"\"\"YOLO model training and evaluation class.\"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model_path: Path):\n",
    "        \"\"\"Initialize trainer with pretrained model.\"\"\"\n",
    "        self.pretrained_model_path = pretrained_model_path\n",
    "        self.models: Dict[str, YOLO] = {}\n",
    "        self.training_results: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 1) TRAIN\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def train_model(\n",
    "        self,\n",
    "        dataset_config: Dict[str, Any],\n",
    "        dataset_name: str,\n",
    "        training_config: Dict[str, Any],\n",
    "    ) -> Dict[str, Any] | None:\n",
    "        \"\"\"Train YOLO model on specified dataset.\"\"\"\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"ğŸš€ Training YOLO model on {dataset_name} dataset\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "\n",
    "        # Fix OpenMP duplicate symbol issue on Windows\n",
    "        os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "        model = YOLO(str(self.pretrained_model_path))\n",
    "        yaml_file = dataset_config[\"yaml_file\"]\n",
    "        project_dir = f\"runs/{dataset_name.lower().replace(' ', '_')}\"\n",
    "\n",
    "        print(f\"ğŸ“„ Dataset config: {yaml_file}\")\n",
    "        print(f\"ğŸ“ Results will be saved to: {project_dir}\")\n",
    "        print(\"âš™ï¸  Training parameters:\")\n",
    "        for key, value in training_config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "\n",
    "        try:\n",
    "            # -------------------- start training -------------------- #\n",
    "            results = model.train(\n",
    "                data=str(yaml_file),\n",
    "                epochs=training_config[\"epochs\"],\n",
    "                batch=training_config[\"batch_size\"],\n",
    "                imgsz=training_config[\"image_size\"],\n",
    "                lr0=training_config[\"learning_rate\"],\n",
    "                patience=training_config[\"patience\"],\n",
    "                save_period=training_config[\"save_period\"],\n",
    "                workers=training_config[\"workers\"],\n",
    "                project=project_dir,\n",
    "                plots=True,\n",
    "                optimizer=\"auto\",\n",
    "                warmup_epochs=5,\n",
    "                cos_lr=True,\n",
    "                hsv_h=0.015,\n",
    "                hsv_s=0.7,\n",
    "                hsv_v=0.4,\n",
    "                degrees=10.0,\n",
    "                translate=0.1,\n",
    "                scale=0.5,\n",
    "                fliplr=0.5,\n",
    "                dropout=0.1,\n",
    "                device=0 if torch.cuda.is_available() else \"cpu\",\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "            # -------------------- bookkeeping -------------------- #\n",
    "            self.models[dataset_name] = model\n",
    "            self.training_results[dataset_name] = {\n",
    "                \"model\": model,\n",
    "                \"results\": results,\n",
    "                \"project_dir\": project_dir,\n",
    "                \"best_model_path\": model.ckpt_path,\n",
    "            }\n",
    "\n",
    "            print(f\"âœ… Training completed for {dataset_name}!\")\n",
    "            print(f\"ğŸ“„ Best model saved to: {model.ckpt_path}\")\n",
    "\n",
    "            return self.training_results[dataset_name]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Training failed for {dataset_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 2) EVALUATE\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def evaluate_model(self, dataset_name: str) -> Dict[str, float] | None:\n",
    "        \"\"\"Evaluate trained model performance.\"\"\"\n",
    "        if dataset_name not in self.training_results:\n",
    "            print(f\"âŒ No training results found for {dataset_name}\")\n",
    "            return None\n",
    "\n",
    "        results = self.training_results[dataset_name][\"results\"]\n",
    "\n",
    "        metrics = {\n",
    "            \"mAP50\": float(results.results_dict.get(\"metrics/mAP50(B)\", 0)),\n",
    "            \"mAP50-95\": float(results.results_dict.get(\"metrics/mAP50-95(B)\", 0)),\n",
    "            \"precision\": float(results.results_dict.get(\"metrics/precision(B)\", 0)),\n",
    "            \"recall\": float(results.results_dict.get(\"metrics/recall(B)\", 0)),\n",
    "            \"box_loss\": float(results.results_dict.get(\"train/box_loss\", 0)),\n",
    "            \"cls_loss\": float(results.results_dict.get(\"train/cls_loss\", 0)),\n",
    "            \"dfl_loss\": float(results.results_dict.get(\"train/dfl_loss\", 0)),\n",
    "        }\n",
    "\n",
    "        print(f\"\\nğŸ“Š {dataset_name} Model Performance:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"   {metric}: {value:.4f}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 3) COMPARE MANY MODELS\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def compare_models(self) -> pd.DataFrame | None:\n",
    "        \"\"\"Compare performance between all trained models.\"\"\"\n",
    "        if len(self.training_results) < 2:\n",
    "            print(\"âŒ Need at least 2 trained models for comparison\")\n",
    "            return None\n",
    "\n",
    "        comparison_data = []\n",
    "\n",
    "        for dataset_name in self.training_results.keys():\n",
    "            metrics = self.evaluate_model(dataset_name)\n",
    "            if metrics:\n",
    "                metrics[\"Dataset\"] = dataset_name\n",
    "                comparison_data.append(metrics)\n",
    "\n",
    "        df = pd.DataFrame(comparison_data)\n",
    "\n",
    "        if not df.empty:\n",
    "            cols = [\"Dataset\"] + [c for c in df.columns if c != \"Dataset\"]\n",
    "            df = df[cols]\n",
    "\n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(\"ğŸ“Š MODEL COMPARISON\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(df.to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "            best_map50 = df.loc[df[\"mAP50\"].idxmax()]\n",
    "            print(\n",
    "                f\"\\nğŸ† Best performing model (mAP50): \"\n",
    "                f\"{best_map50['Dataset']} ({best_map50['mAP50']:.4f})\"\n",
    "            )\n",
    "\n",
    "        return df\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 4) VISUALIZE TRAINING CURVES\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def visualize_training_curves(self, dataset_name: str):\n",
    "        \"\"\"Display training curves for a specific model.\"\"\"\n",
    "        if dataset_name not in self.training_results:\n",
    "            print(f\"âŒ No training results found for {dataset_name}\")\n",
    "            return\n",
    "\n",
    "        project_dir = Path(self.training_results[dataset_name][\"project_dir\"])\n",
    "        results_img = project_dir / \"train\" / \"results.png\"\n",
    "\n",
    "        if results_img.exists():\n",
    "            print(f\"\\nğŸ“ˆ Training curves for {dataset_name}:\")\n",
    "            display(Image(filename=str(results_img)))\n",
    "        else:\n",
    "            print(f\"âŒ Training curves not found at {results_img}\")\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 5) QUICK INFERENCE TEST\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def test_inference(\n",
    "        self, dataset_name: str, test_image_path: Path, conf_threshold: float = 0.25\n",
    "    ):\n",
    "        \"\"\"Test model inference on a sample image.\"\"\"\n",
    "        if dataset_name not in self.models:\n",
    "            print(f\"âŒ No trained model found for {dataset_name}\")\n",
    "            return\n",
    "\n",
    "        if not test_image_path.exists():\n",
    "            print(f\"âŒ Test image not found: {test_image_path}\")\n",
    "            return\n",
    "\n",
    "        model = self.models[dataset_name]\n",
    "\n",
    "        print(f\"\\nğŸ” Testing {dataset_name} model on {test_image_path.name}\")\n",
    "\n",
    "        results = model.predict(source=str(test_image_path), conf=conf_threshold, save=True)\n",
    "\n",
    "        if results and len(results) > 0:\n",
    "            result = results[0]\n",
    "            if hasattr(result, \"save_dir\"):\n",
    "                result_image = Path(result.save_dir) / test_image_path.name\n",
    "                if result_image.exists():\n",
    "                    print(\"\\nğŸ“¸ Inference result:\")\n",
    "                    display(Image(filename=str(result_image)))\n",
    "                else:\n",
    "                    print(f\"âŒ Result image not found at {result_image}\")\n",
    "\n",
    "            if hasattr(result, \"boxes\") and result.boxes is not None:\n",
    "                print(f\"\\nğŸ¯ Detected {len(result.boxes)} objects\")\n",
    "                for i, box in enumerate(result.boxes):\n",
    "                    conf = float(box.conf)\n",
    "                    cls = int(box.cls)\n",
    "                    print(\n",
    "                        f\"   Detection {i + 1}: class={cls} (person), \"\n",
    "                        f\"confidence={conf:.3f}\"\n",
    "                    )\n",
    "        else:\n",
    "            print(\"âŒ No inference results returned\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "PRETRAINED_MODEL_PATH = Path(\"yolo12x.pt\")\n",
    "trainer = YOLOTrainer(PRETRAINED_MODEL_PATH)\n",
    "print(\"âœ… YOLOTrainer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff1822",
   "metadata": {},
   "source": [
    "## 6. Train Models on Both Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055ed54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models on both datasets\n",
    "# Dictionary lÆ°u láº¡i thÃ´ng tin huáº¥n luyá»‡n cá»§a tá»«ng táº­p\n",
    "training_results = {}\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Kiá»ƒm tra xem Ä‘Ã£ cÃ³ dá»¯ liá»‡u chÆ°a\n",
    "# --------------------------------------------------\n",
    "if not datasets:\n",
    "    print(\"âŒ No datasets prepared. Please run the dataset preparation cells first.\")\n",
    "else:\n",
    "    print(f\"ğŸ¯ Starting training on {len(datasets)} datasets...\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1) Huáº¥n luyá»‡n trÃªn táº­p Original\n",
    "    # --------------------------------------------------\n",
    "    if \"original\" in datasets:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸš€ Training Model 1: Original Images Dataset\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        original_result = trainer.train_model(\n",
    "            dataset_config=datasets[\"original\"],\n",
    "            dataset_name=\"Original Images\",\n",
    "            training_config=TRAINING_CONFIG,\n",
    "        )\n",
    "\n",
    "        if original_result:\n",
    "            training_results[\"original\"] = original_result\n",
    "            print(\"âœ… Original dataset training completed!\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2) Huáº¥n luyá»‡n trÃªn táº­p Background-Fused\n",
    "    # --------------------------------------------------\n",
    "    if \"fused\" in datasets:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸš€ Training Model 2: Background-Fused Dataset\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        fused_result = trainer.train_model(\n",
    "            dataset_config=datasets[\"fused\"],\n",
    "            dataset_name=\"Background Fused\",\n",
    "            training_config=TRAINING_CONFIG,\n",
    "        )\n",
    "\n",
    "        if fused_result:\n",
    "            training_results[\"fused\"] = fused_result\n",
    "            print(\"âœ… Fused dataset training completed!\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # TÃ³m táº¯t\n",
    "    # --------------------------------------------------\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ğŸ‰ All training completed!\")\n",
    "    print(f\"âœ… Trained {len(training_results)} models successfully\")\n",
    "    print(f\"{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695eeab",
   "metadata": {},
   "source": [
    "## 7. Train Models on Subsampled Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4498a7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Subsampled Training Configurations:\n",
      "  â€¢ 500 images:\n",
      "      epochs: 200\n",
      "      batch_size: 6\n",
      "      image_size: 640\n",
      "      learning_rate: 0.0005\n",
      "      patience: 40\n",
      "      save_period: 5\n",
      "      workers: 8\n",
      "  â€¢ 1000 images:\n",
      "      epochs: 150\n",
      "      batch_size: 8\n",
      "      image_size: 640\n",
      "      learning_rate: 0.0007\n",
      "      patience: 30\n",
      "      save_period: 5\n",
      "      workers: 8\n",
      "  â€¢ 2000 images:\n",
      "      epochs: 100\n",
      "      batch_size: 16\n",
      "      image_size: 768\n",
      "      learning_rate: 0.001\n",
      "      patience: 25\n",
      "      save_period: 5\n",
      "      workers: 12\n",
      "  â€¢ 5000 images:\n",
      "      epochs: 100\n",
      "      batch_size: 16\n",
      "      image_size: 1024\n",
      "      learning_rate: 0.001\n",
      "      patience: 20\n",
      "      save_period: 10\n",
      "      workers: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# SUBSAMPLED TRAINING CONFIGURATION PER SIZE\n",
    "# =========================\n",
    "\n",
    "SUBSAMPLE_TRAINING_CONFIGS = {\n",
    "    500: {\n",
    "        'epochs':        200,\n",
    "        'batch_size':    6,\n",
    "        'image_size':    640,\n",
    "        'learning_rate': 5e-4,\n",
    "        'patience':      40,\n",
    "        'save_period':   5,\n",
    "        'workers':       8,\n",
    "    },\n",
    "    1000: {\n",
    "        'epochs':        150,\n",
    "        'batch_size':    8,\n",
    "        'image_size':    640,\n",
    "        'learning_rate': 7e-4,\n",
    "        'patience':      30,\n",
    "        'save_period':   5,\n",
    "        'workers':       8,\n",
    "    },\n",
    "    2000: {\n",
    "        'epochs':        100,\n",
    "        'batch_size':    8,\n",
    "        'accumulate':    2,\n",
    "        'image_size':    640,\n",
    "        'learning_rate': 1e-3,\n",
    "        'patience':      25,\n",
    "        'save_period':   5,\n",
    "        'workers':       12,\n",
    "    },\n",
    "    5000: {\n",
    "        'epochs':        100,\n",
    "        'batch_size':    16,\n",
    "        'image_size':    1024,\n",
    "        'learning_rate': 1e-3,\n",
    "        'patience':      20,\n",
    "        'save_period':   10,\n",
    "        'workers':       8,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸ¯ Subsampled Training Configurations:\")\n",
    "for size, cfg in SUBSAMPLE_TRAINING_CONFIGS.items():\n",
    "    print(f\"  â€¢ {size} images:\")\n",
    "    for k, v in cfg.items():\n",
    "        print(f\"      {k}: {v}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1bc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_training_results = {}\n",
    "\n",
    "if not subsampled_datasets:\n",
    "    print(\"âŒ No subsampled datasets prepared. Please run the subsampling cells first.\")\n",
    "else:\n",
    "    total_models_to_train = sum(len(sub) for sub in subsampled_datasets.values())\n",
    "    print(f\"ğŸ¯ Starting training on {total_models_to_train} subsampled datasets...\\n\")\n",
    "    \n",
    "    trained_count = 0\n",
    "    \n",
    "    for dataset_type, subsamples in subsampled_datasets.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ğŸš€ Training {dataset_type.upper()} subsampled models\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        subsample_training_results[dataset_type] = {}\n",
    "        \n",
    "        for sample_size, dataset_info in subsamples.items():\n",
    "            trained_count += 1\n",
    "            model_name = f\"{dataset_type}_{sample_size}\"\n",
    "            cfg = SUBSAMPLE_TRAINING_CONFIGS.get(sample_size)\n",
    "            \n",
    "            print(f\"ğŸ”¸ [{trained_count}/{total_models_to_train}] Training {model_name}\")\n",
    "            print(f\"   Dataset: {sample_size} images \"\n",
    "                  f\"({dataset_info['positive_samples']} pos, {dataset_info['negative_samples']} neg)\")\n",
    "            print(f\"   Using config for {sample_size} images:\")\n",
    "            for k, v in cfg.items():\n",
    "                print(f\"      {k}: {v}\")\n",
    "            \n",
    "            # Train the model with the size-specific config\n",
    "            result = trainer.train_model(\n",
    "                dataset_config=dataset_info,\n",
    "                dataset_name=model_name,\n",
    "                training_config=cfg,\n",
    "            )\n",
    "            \n",
    "            if result:\n",
    "                subsample_training_results[dataset_type][sample_size] = result\n",
    "                print(f\"âœ… {model_name} training completed!\\n\")\n",
    "            else:\n",
    "                print(f\"âŒ {model_name} training failed!\\n\")\n",
    "    \n",
    "    # Summary\n",
    "    total_successful = sum(len(r) for r in subsample_training_results.values())\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ğŸ‰ Subsampled dataset training completed!\")\n",
    "    print(f\"âœ… Successfully trained {total_successful}/{total_models_to_train} models\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b413a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA devices:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current device:\", torch.cuda.current_device(), \"-\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "n_workers = max(1, os.cpu_count() // 2)  \n",
    "print(f\"Setting DataLoader workers = {n_workers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96112a8",
   "metadata": {},
   "source": [
    "## 7.1. Selective Training - Run Specific Subsample Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SELECTIVE TRAINING ON 2000 IMAGES\n",
    "# =========================\n",
    "\n",
    "# Specify which sizes to train (start with 2000)\n",
    "SIZES_TO_TRAIN = [2000]  # Add more sizes like [2000, 1000, 500] as needed\n",
    "\n",
    "# Initialize results storage if not already done\n",
    "if 'subsample_training_results' not in locals():\n",
    "    subsample_training_results = {}\n",
    "\n",
    "if not subsampled_datasets:\n",
    "    print(\"âŒ No subsampled datasets prepared. Please run the subsampling cells first.\")\n",
    "else:\n",
    "    # Filter to only train on specified sizes\n",
    "    models_to_train = []\n",
    "    for dataset_type in ['original', 'fused']:\n",
    "        if dataset_type in subsampled_datasets:\n",
    "            for sample_size in SIZES_TO_TRAIN:\n",
    "                if sample_size in subsampled_datasets[dataset_type]:\n",
    "                    models_to_train.append((dataset_type, sample_size))\n",
    "    \n",
    "    if not models_to_train:\n",
    "        print(f\"âŒ None of the specified sizes {SIZES_TO_TRAIN} are available in subsampled datasets.\")\n",
    "        print(\"Available sizes:\")\n",
    "        for dataset_type, subsamples in subsampled_datasets.items():\n",
    "            print(f\"  {dataset_type}: {list(subsamples.keys())}\")\n",
    "    else:\n",
    "        print(f\"ğŸ¯ Training on {len(models_to_train)} models with sizes: {SIZES_TO_TRAIN}\")\n",
    "        print(f\"ğŸ“‹ Models to train: {[f'{dt}_{size}' for dt, size in models_to_train]}\")\n",
    "        \n",
    "        # Train each model\n",
    "        for i, (dataset_type, sample_size) in enumerate(models_to_train, 1):\n",
    "            model_name = f\"{dataset_type}_{sample_size}\"\n",
    "            dataset_info = subsampled_datasets[dataset_type][sample_size]\n",
    "            cfg = SUBSAMPLE_TRAINING_CONFIGS.get(sample_size)\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"ğŸš€ [{i}/{len(models_to_train)}] Training {model_name}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"ğŸ“Š Dataset: {sample_size} images \"\n",
    "                  f\"({dataset_info['positive_samples']} pos, {dataset_info['negative_samples']} neg)\")\n",
    "            print(f\"âš™ï¸  Training configuration:\")\n",
    "            for k, v in cfg.items():\n",
    "                print(f\"      {k}: {v}\")\n",
    "            print()\n",
    "            \n",
    "            # Initialize nested dict if needed\n",
    "            if dataset_type not in subsample_training_results:\n",
    "                subsample_training_results[dataset_type] = {}\n",
    "            \n",
    "            # Train the model\n",
    "            result = trainer.train_model(\n",
    "                dataset_config=dataset_info,\n",
    "                dataset_name=model_name,\n",
    "                training_config=cfg,\n",
    "            )\n",
    "            \n",
    "            if result:\n",
    "                subsample_training_results[dataset_type][sample_size] = result\n",
    "                print(f\"âœ… {model_name} training completed successfully!\")\n",
    "                \n",
    "                # Show quick evaluation\n",
    "                metrics = trainer.evaluate_model(model_name)\n",
    "                if metrics:\n",
    "                    print(f\"ğŸ“Š Quick Results: mAP50={metrics['mAP50']:.4f}, \"\n",
    "                          f\"Precision={metrics['precision']:.4f}, \"\n",
    "                          f\"Recall={metrics['recall']:.4f}\")\n",
    "            else:\n",
    "                print(f\"âŒ {model_name} training failed!\")\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        # Summary\n",
    "        successful_models = []\n",
    "        for dataset_type, sample_size in models_to_train:\n",
    "            if (dataset_type in subsample_training_results and \n",
    "                sample_size in subsample_training_results[dataset_type]):\n",
    "                successful_models.append(f\"{dataset_type}_{sample_size}\")\n",
    "        \n",
    "        print(f\"{'='*70}\")\n",
    "        print(\"ğŸ‰ Selective Training Summary\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"âœ… Successfully trained: {len(successful_models)}/{len(models_to_train)} models\")\n",
    "        print(f\"ğŸ“‹ Completed models: {successful_models}\")\n",
    "        \n",
    "        if len(successful_models) >= 2:\n",
    "            print(\"\\nğŸ’¡ Next steps:\")\n",
    "            print(\"1. Run the evaluation section to compare results\")\n",
    "            print(\"2. Add more sizes to SIZES_TO_TRAIN if satisfied with results\")\n",
    "            print(\"3. Use the analysis section to visualize performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08829f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Re-running training for fused_2000\n",
      "   Dataset directory: datasets\\yolo_fused_sub2000\n",
      "   Config file: datasets\\yolo_fused_sub2000\\yolo_fused_sub2000.yaml\n",
      "   Training config:\n",
      "      epochs: 100\n",
      "      batch_size: 16\n",
      "      image_size: 768\n",
      "      learning_rate: 0.001\n",
      "      patience: 25\n",
      "      save_period: 5\n",
      "      workers: 12\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Training YOLO model on fused_2000 dataset\n",
      "============================================================\n",
      "ğŸ“„ Dataset config: datasets\\yolo_fused_sub2000\\yolo_fused_sub2000.yaml\n",
      "ğŸ“ Results will be saved to: runs/fused_2000\n",
      "âš™ï¸  Training parameters:\n",
      "   epochs: 100\n",
      "   batch_size: 16\n",
      "   image_size: 768\n",
      "   learning_rate: 0.001\n",
      "   patience: 25\n",
      "   save_period: 5\n",
      "   workers: 12\n",
      "New https://pypi.org/project/ultralytics/8.3.176 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.174  Python-3.10.18 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU, 16384MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=datasets\\yolo_fused_sub2000\\yolo_fused_sub2000.yaml, degrees=10.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.1, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=768, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12x.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train5, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=25, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/fused_2000, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\fused_2000\\train5, save_frames=False, save_json=False, save_period=5, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=5, warmup_momentum=0.8, weight_decay=0.0005, workers=12, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2784  ultralytics.nn.modules.conv.Conv             [3, 96, 3, 2]                 \n",
      "  1                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  2                  -1  2    389760  ultralytics.nn.modules.block.C3k2            [192, 384, 2, True, 0.25]     \n",
      "  3                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      "  4                  -1  2   1553664  ultralytics.nn.modules.block.C3k2            [384, 768, 2, True, 0.25]     \n",
      "  5                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
      "  6                  -1  4   9512128  ultralytics.nn.modules.block.A2C2f           [768, 768, 4, True, 4, True, 1.2]\n",
      "  7                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
      "  8                  -1  4   9512128  ultralytics.nn.modules.block.A2C2f           [768, 768, 4, True, 1, True, 1.2]\n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  2   4727040  ultralytics.nn.modules.block.A2C2f           [1536, 768, 2, False, -1, True, 1.2]\n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  2   1331328  ultralytics.nn.modules.block.A2C2f           [1536, 384, 2, False, -1, True, 1.2]\n",
      " 15                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  2   4579584  ultralytics.nn.modules.block.A2C2f           [1152, 768, 2, False, -1, True, 1.2]\n",
      " 18                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]          \n",
      " 21        [14, 17, 20]  1   3146707  ultralytics.nn.modules.head.Detect           [1, [384, 768, 768]]          \n",
      "YOLOv12x summary: 488 layers, 59,119,539 parameters, 59,119,523 gradients, 199.8 GFLOPs\n",
      "\n",
      "Transferred 1239/1245 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 652.4124.3 MB/s, size: 369.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\auto-labeling\\src\\datasets\\yolo_fused_sub2000\\labels\\train.cache... 1600 images, 19 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 361.4146.4 MB/s, size: 348.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\auto-labeling\\src\\datasets\\yolo_fused_sub2000\\labels\\val.cache... 400 images, 4 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\fused_2000\\train5\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 205 weight(decay=0.0), 214 weight(decay=0.0005), 211 bias(decay=0.0)\n",
      "Image sizes 768 train, 768 val\n",
      "Using 12 dataloader workers\n",
      "Logging results to \u001b[1mruns\\fused_2000\\train5\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:30<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Training failed for fused_2000: CUDA out of memory. Tried to allocate 486.00 MiB. GPU 0 has a total capacity of 16.00 GiB of which 0 bytes is free. Of the allocated memory 23.97 GiB is allocated by PyTorch, and 404.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "âŒ fused_2000 training failed!\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# RE-RUN TRAINING FOR FUSED 2000-IMAGE DATASET (DIRECT LOAD)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Set paths directly\n",
    "fused_sub_dir = Path(\"datasets/yolo_fused_sub2000\")\n",
    "yaml_file = fused_sub_dir / \"yolo_fused_sub2000.yaml\"\n",
    "\n",
    "# Minimal dataset_info dict for trainer.train_model\n",
    "dataset_info = {\n",
    "    \"yaml_file\": yaml_file,\n",
    "    \"output_dir\": fused_sub_dir,\n",
    "    # Optionally, add dummy values for display\n",
    "    \"positive_samples\": \"?\", \n",
    "    \"negative_samples\": \"?\", \n",
    "}\n",
    "\n",
    "# Load config for 2000 images\n",
    "cfg = SUBSAMPLE_TRAINING_CONFIGS.get(2000)\n",
    "model_name = \"fused_2000\"\n",
    "\n",
    "print(f\"ğŸš€ Re-running training for {model_name}\")\n",
    "print(f\"   Dataset directory: {fused_sub_dir}\")\n",
    "print(f\"   Config file: {yaml_file}\")\n",
    "print(\"   Training config:\")\n",
    "for k, v in cfg.items():\n",
    "    print(f\"      {k}: {v}\")\n",
    "print()\n",
    "\n",
    "# Train the model\n",
    "result = trainer.train_model(\n",
    "    dataset_config=dataset_info,\n",
    "    dataset_name=model_name,\n",
    "    training_config=cfg,\n",
    ")\n",
    "\n",
    "if result:\n",
    "    if \"subsample_training_results\" not in locals():\n",
    "        subsample_training_results = {}\n",
    "    if \"fused\" not in subsample_training_results:\n",
    "        subsample_training_results[\"fused\"] = {}\n",
    "    subsample_training_results[\"fused\"][2000] = result\n",
    "    print(f\"âœ… {model_name} training completed successfully!\")\n",
    "    metrics = trainer.evaluate_model(model_name)\n",
    "    if metrics:\n",
    "        print(f\"ğŸ“Š Quick Results: mAP50={metrics['mAP50']:.4f}, \"\n",
    "              f\"Precision={metrics['precision']:.4f}, \"\n",
    "              f\"Recall={metrics['recall']:.4f}\")\n",
    "else:\n",
    "    print(f\"âŒ {model_name} training failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b794cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# QUICK EVALUATION OF 2000-IMAGE MODELS\n",
    "# =========================\n",
    "\n",
    "# Check if we have trained models for the 2000-image size\n",
    "trained_2000_models = []\n",
    "for dataset_type in ['original', 'fused']:\n",
    "    model_name = f\"{dataset_type}_2000\"\n",
    "    if model_name in trainer.models:\n",
    "        trained_2000_models.append(model_name)\n",
    "\n",
    "if trained_2000_models:\n",
    "    print(f\"ğŸ“Š Quick evaluation of {len(trained_2000_models)} trained 2000-image models\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name in trained_2000_models:\n",
    "        print(f\"\\nğŸ” Evaluating {model_name}...\")\n",
    "        metrics = trainer.evaluate_model(model_name)\n",
    "        if metrics:\n",
    "            metrics['Model'] = model_name\n",
    "            comparison_data.append(metrics)\n",
    "    \n",
    "    if comparison_data:\n",
    "        import pandas as pd\n",
    "        df_2000 = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # Reorder columns\n",
    "        cols = ['Model'] + [c for c in df_2000.columns if c != 'Model']\n",
    "        df_2000 = df_2000[cols]\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ 2000-Image Model Comparison:\")\n",
    "        print(\"=\" * 70)\n",
    "        print(df_2000.to_string(index=False, float_format='%.4f'))\n",
    "        \n",
    "        # Find best model\n",
    "        if len(df_2000) > 1:\n",
    "            best_model = df_2000.loc[df_2000['mAP50'].idxmax()]\n",
    "            print(f\"\\nğŸ† Best 2000-image model: {best_model['Model']} (mAP50: {best_model['mAP50']:.4f})\")\n",
    "            \n",
    "            # Compare original vs fused\n",
    "            original_row = df_2000[df_2000['Model'].str.contains('original')]\n",
    "            fused_row = df_2000[df_2000['Model'].str.contains('fused')]\n",
    "            \n",
    "            if not original_row.empty and not fused_row.empty:\n",
    "                orig_map50 = original_row['mAP50'].iloc[0]\n",
    "                fused_map50 = fused_row['mAP50'].iloc[0]\n",
    "                improvement = fused_map50 - orig_map50\n",
    "                \n",
    "                print(f\"\\nğŸ“ˆ Background Fusion Impact:\")\n",
    "                print(f\"   Original: {orig_map50:.4f} mAP50\")\n",
    "                print(f\"   Fused:    {fused_map50:.4f} mAP50\")\n",
    "                print(f\"   Change:   {improvement:+.4f} mAP50 ({improvement/orig_map50*100:+.1f}%)\")\n",
    "        \n",
    "        # Save results\n",
    "        results_file = OUTPUT_BASE_DIR / \"2000_image_model_comparison.csv\"\n",
    "        df_2000.to_csv(results_file, index=False)\n",
    "        print(f\"\\nğŸ’¾ Results saved to: {results_file}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ No 2000-image models found. Please run the training cell above first.\")\n",
    "    print(\"ğŸ’¡ Make sure SIZES_TO_TRAIN includes 2000 and run the training cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c8793",
   "metadata": {},
   "source": [
    "## 8. Evaluate and Compare Models\n",
    "\n",
    "Let's evaluate the performance of both models and compare their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5844ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Evaluate model performance for all trained models\n",
    "# --------------------------------------------------\n",
    "if trainer.training_results:\n",
    "    print(\"ğŸ“Š Evaluating model performance...\")\n",
    "\n",
    "    # Build a comparison DataFrame\n",
    "    comparison_df = trainer.compare_models()\n",
    "\n",
    "    if comparison_df is not None and not comparison_df.empty:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # Create a 2 Ã— 2 subplot grid\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(\"Model Performance Comparison\", fontsize=16)\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # 1) mAP@0.5\n",
    "        # --------------------------------------------------\n",
    "        axes[0, 0].bar(comparison_df[\"Dataset\"], comparison_df[\"mAP50\"])\n",
    "        axes[0, 0].set_title(\"mAP@0.5\")\n",
    "        axes[0, 0].set_ylabel(\"mAP50\")\n",
    "        axes[0, 0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # 2) mAP@0.5:0.95\n",
    "        # --------------------------------------------------\n",
    "        axes[0, 1].bar(comparison_df[\"Dataset\"], comparison_df[\"mAP50-95\"])\n",
    "        axes[0, 1].set_title(\"mAP@0.5:0.95\")\n",
    "        axes[0, 1].set_ylabel(\"mAP50-95\")\n",
    "        axes[0, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # 3) Precision vs Recall\n",
    "        # --------------------------------------------------\n",
    "        axes[1, 0].scatter(\n",
    "            comparison_df[\"recall\"],\n",
    "            comparison_df[\"precision\"],\n",
    "            s=100,\n",
    "            zorder=3,\n",
    "        )\n",
    "        for i, dataset in enumerate(comparison_df[\"Dataset\"]):\n",
    "            axes[1, 0].annotate(\n",
    "                dataset,\n",
    "                (comparison_df[\"recall\"].iloc[i], comparison_df[\"precision\"].iloc[i]),\n",
    "                xytext=(5, 5),\n",
    "                textcoords=\"offset points\",\n",
    "            )\n",
    "        axes[1, 0].set_xlabel(\"Recall\")\n",
    "        axes[1, 0].set_ylabel(\"Precision\")\n",
    "        axes[1, 0].set_title(\"Precision vs Recall\")\n",
    "        axes[1, 0].grid(True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # 4) Losses (box, cls, dfl)\n",
    "        # --------------------------------------------------\n",
    "        loss_cols = [\"box_loss\", \"cls_loss\", \"dfl_loss\"]\n",
    "        x = range(len(comparison_df))\n",
    "        width = 0.25\n",
    "\n",
    "        axes[1, 1].bar([i - width for i in x], comparison_df[\"box_loss\"],  width, label=\"Box Loss\")\n",
    "        axes[1, 1].bar([i for i in x],          comparison_df[\"cls_loss\"],  width, label=\"Class Loss\")\n",
    "        axes[1, 1].bar([i + width for i in x],  comparison_df[\"dfl_loss\"],  width, label=\"DFL Loss\")\n",
    "\n",
    "        axes[1, 1].set_xlabel(\"Dataset\")\n",
    "        axes[1, 1].set_ylabel(\"Loss\")\n",
    "        axes[1, 1].set_title(\"Training Losses\")\n",
    "        axes[1, 1].set_xticks(x)\n",
    "        axes[1, 1].set_xticklabels(comparison_df[\"Dataset\"])\n",
    "        axes[1, 1].legend()\n",
    "\n",
    "        # Improve layout\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # Save the comparison table\n",
    "        # --------------------------------------------------\n",
    "        comparison_file = OUTPUT_BASE_DIR / \"model_comparison.csv\"\n",
    "        comparison_df.to_csv(comparison_file, index=False)\n",
    "        print(f\"\\nğŸ’¾ Model comparison saved to: {comparison_file}\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No trained models found. Please run the training cells first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2263e6cf",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Curves\n",
    "\n",
    "Display training curves for both models to understand training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0debf233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training curves for all models\n",
    "if trainer.training_results:\n",
    "    for dataset_name in trainer.training_results:\n",
    "        trainer.visualize_training_curves(dataset_name)\n",
    "else:\n",
    "    print(\"âŒ No training results available for visualization.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d04c415",
   "metadata": {},
   "source": [
    "## 10. Test Model Inference\n",
    "\n",
    "Test both models on sample images to see their detection performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800bf3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference on sample images\n",
    "if trainer.models and datasets:\n",
    "    print(\"ğŸ” Testing model inference...\")\n",
    "\n",
    "    # Find sample test images from validation sets\n",
    "    test_images = []\n",
    "\n",
    "    for dataset_name, dataset_info in datasets.items():\n",
    "        val_images_dir = dataset_info['output_dir'] / 'images' / 'val'\n",
    "        if val_images_dir.exists():\n",
    "            # Get first 2 validation images as test samples\n",
    "            sample_images = list(val_images_dir.glob('*.jpg'))[:2]\n",
    "            for img in sample_images:\n",
    "                test_images.append((dataset_name, img))\n",
    "\n",
    "    if test_images:\n",
    "        print(f\"\\nğŸ“¸ Testing on {len(test_images)} sample images...\")\n",
    "\n",
    "        # Test each model on sample images\n",
    "        conf_threshold = 0.3\n",
    "\n",
    "        for model_name in trainer.models.keys():\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"ğŸ¤– Testing {model_name} model\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "            for i, (dataset_source, test_image) in enumerate(test_images[:2]):  # Test on first 2 images\n",
    "                print(f\"\\nğŸ“· Test image {i+1}: {test_image.name} (from {dataset_source} dataset)\")\n",
    "                trainer.test_inference(model_name, test_image, conf_threshold)\n",
    "\n",
    "                # Add some spacing between results\n",
    "                print(\"\\n\" + \"-\"*40)\n",
    "\n",
    "    else:\n",
    "        print(\"âŒ No test images found in validation sets.\")\n",
    "        print(\"ğŸ’¡ You can manually specify a test image path in the next cell.\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No trained models or datasets available for testing.\")\n",
    "    print(\"ğŸ’¡ Please run the dataset preparation and training cells first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de9449",
   "metadata": {},
   "source": [
    "## 10. Custom Inference Testing (Optional)\n",
    "\n",
    "Test your models on custom images by specifying the path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb4bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom inference testing - modify the path below\n",
    "CUSTOM_TEST_IMAGE_PATH = Path(\"path/to/your/test/image.jpg\")  # Update this path\n",
    "CONF_THRESHOLD = 0.25  # Confidence threshold for detections\n",
    "\n",
    "# Test custom image if path is provided and exists\n",
    "if CUSTOM_TEST_IMAGE_PATH.exists() and trainer.models:\n",
    "    print(f\"ğŸ” Testing custom image: {CUSTOM_TEST_IMAGE_PATH}\")\n",
    "\n",
    "    # Test all trained models on the custom image\n",
    "    for model_name in trainer.models.keys():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ğŸ¤– {model_name} Model Results\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        trainer.test_inference(model_name, CUSTOM_TEST_IMAGE_PATH, CONF_THRESHOLD)\n",
    "\n",
    "else:\n",
    "    if not CUSTOM_TEST_IMAGE_PATH.exists():\n",
    "        print(\"ğŸ’¡ To test a custom image:\")\n",
    "        print(\"   1. Update CUSTOM_TEST_IMAGE_PATH variable above\")\n",
    "        print(\"   2. Make sure the image file exists\")\n",
    "        print(\"   3. Re-run this cell\")\n",
    "\n",
    "    if not trainer.models:\n",
    "        print(\"âŒ No trained models available for testing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d75d7",
   "metadata": {},
   "source": [
    "## 11. Analyze Subsampled Model Performance\n",
    "\n",
    "Let's analyze how dataset size affects model performance by comparing results across different subsample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93900f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ANALYZE SUBSAMPLED MODEL PERFORMANCE\n",
    "# =========================\n",
    "\n",
    "if subsample_training_results:\n",
    "    print(\"ğŸ“Š Analyzing subsampled model performance...\")\n",
    "    \n",
    "    # Collect performance data for all subsampled models\n",
    "    subsample_performance_data = []\n",
    "    \n",
    "    for dataset_type, results in subsample_training_results.items():\n",
    "        for sample_size, result in results.items():\n",
    "            model_name = f\"{dataset_type}_{sample_size}\"\n",
    "            \n",
    "            # Evaluate this model\n",
    "            metrics = trainer.evaluate_model(model_name)\n",
    "            if metrics:\n",
    "                metrics.update({\n",
    "                    'dataset_type': dataset_type,\n",
    "                    'sample_size': sample_size,\n",
    "                    'model_name': model_name\n",
    "                })\n",
    "                subsample_performance_data.append(metrics)\n",
    "    \n",
    "    if subsample_performance_data:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        \n",
    "        df_subsample = pd.DataFrame(subsample_performance_data)\n",
    "        \n",
    "        # Create comprehensive analysis plots\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('Impact of Dataset Size on YOLO Performance', fontsize=16)\n",
    "        \n",
    "        # Plot for each dataset type\n",
    "        for dataset_type in ['original', 'fused']:\n",
    "            if dataset_type in df_subsample['dataset_type'].values:\n",
    "                subset = df_subsample[df_subsample['dataset_type'] == dataset_type].sort_values('sample_size')\n",
    "                color = 'blue' if dataset_type == 'original' else 'red'\n",
    "                \n",
    "                # mAP@0.5 vs Dataset Size\n",
    "                axes[0, 0].plot(subset['sample_size'], subset['mAP50'], \n",
    "                              marker='o', label=f'{dataset_type.title()}', color=color)\n",
    "                \n",
    "                # mAP@0.5:0.95 vs Dataset Size\n",
    "                axes[0, 1].plot(subset['sample_size'], subset['mAP50-95'], \n",
    "                              marker='o', label=f'{dataset_type.title()}', color=color)\n",
    "                \n",
    "                # Precision vs Dataset Size\n",
    "                axes[0, 2].plot(subset['sample_size'], subset['precision'], \n",
    "                              marker='o', label=f'{dataset_type.title()}', color=color)\n",
    "                \n",
    "                # Recall vs Dataset Size\n",
    "                axes[1, 0].plot(subset['sample_size'], subset['recall'], \n",
    "                              marker='o', label=f'{dataset_type.title()}', color=color)\n",
    "                \n",
    "                # Box Loss vs Dataset Size\n",
    "                axes[1, 1].plot(subset['sample_size'], subset['box_loss'], \n",
    "                              marker='o', label=f'{dataset_type.title()}', color=color)\n",
    "                \n",
    "                # Class Loss vs Dataset Size\n",
    "                axes[1, 2].plot(subset['sample_size'], subset['cls_loss'], \n",
    "                              marker='o', label=f'{dataset_type.title()}', color=color)\n",
    "        \n",
    "        # Configure plots\n",
    "        axes[0, 0].set_title('mAP@0.5 vs Dataset Size')\n",
    "        axes[0, 0].set_xlabel('Dataset Size')\n",
    "        axes[0, 0].set_ylabel('mAP@0.5')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[0, 1].set_title('mAP@0.5:0.95 vs Dataset Size')\n",
    "        axes[0, 1].set_xlabel('Dataset Size')\n",
    "        axes[0, 1].set_ylabel('mAP@0.5:0.95')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[0, 2].set_title('Precision vs Dataset Size')\n",
    "        axes[0, 2].set_xlabel('Dataset Size')\n",
    "        axes[0, 2].set_ylabel('Precision')\n",
    "        axes[0, 2].legend()\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1, 0].set_title('Recall vs Dataset Size')\n",
    "        axes[1, 0].set_xlabel('Dataset Size')\n",
    "        axes[1, 0].set_ylabel('Recall')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1, 1].set_title('Box Loss vs Dataset Size')\n",
    "        axes[1, 1].set_xlabel('Dataset Size')\n",
    "        axes[1, 1].set_ylabel('Box Loss')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1, 2].set_title('Class Loss vs Dataset Size')\n",
    "        axes[1, 2].set_xlabel('Dataset Size')\n",
    "        axes[1, 2].set_ylabel('Class Loss')\n",
    "        axes[1, 2].legend()\n",
    "        axes[1, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed comparison table\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(\"ğŸ“Š DETAILED SUBSAMPLED MODEL COMPARISON\")\n",
    "        print(f\"{'='*100}\")\n",
    "        \n",
    "        # Sort by dataset type and sample size\n",
    "        df_display = df_subsample.sort_values(['dataset_type', 'sample_size'])\n",
    "        \n",
    "        # Select key columns for display\n",
    "        display_cols = ['dataset_type', 'sample_size', 'mAP50', 'mAP50-95', 'precision', 'recall']\n",
    "        print(df_display[display_cols].to_string(index=False, float_format='%.4f'))\n",
    "        \n",
    "        # Find best performing models at each size\n",
    "        print(f\"\\nğŸ† BEST MODELS BY DATASET SIZE:\")\n",
    "        for size in sorted(df_subsample['sample_size'].unique()):\n",
    "            size_data = df_subsample[df_subsample['sample_size'] == size]\n",
    "            best_idx = size_data['mAP50'].idxmax()\n",
    "            best_model = size_data.loc[best_idx]\n",
    "            print(f\"   ğŸ“Š {size} images: {best_model['model_name']} (mAP50: {best_model['mAP50']:.4f})\")\n",
    "        \n",
    "        # Analyze performance trends\n",
    "        print(f\"\\nğŸ“ˆ PERFORMANCE TRENDS:\")\n",
    "        for dataset_type in ['original', 'fused']:\n",
    "            if dataset_type in df_subsample['dataset_type'].values:\n",
    "                subset = df_subsample[df_subsample['dataset_type'] == dataset_type].sort_values('sample_size')\n",
    "                if len(subset) > 1:\n",
    "                    map50_improvement = subset['mAP50'].iloc[-1] - subset['mAP50'].iloc[0]\n",
    "                    size_range = f\"{subset['sample_size'].iloc[0]}-{subset['sample_size'].iloc[-1]}\"\n",
    "                    print(f\"   ğŸ¯ {dataset_type.title()}: mAP50 change from {size_range} images: {map50_improvement:+.4f}\")\n",
    "        \n",
    "        # Save detailed results\n",
    "        subsample_results_file = OUTPUT_BASE_DIR / \"subsample_performance_analysis.csv\"\n",
    "        df_subsample.to_csv(subsample_results_file, index=False)\n",
    "        print(f\"\\nğŸ’¾ Detailed subsampled results saved to: {subsample_results_file}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ No subsampled model performance data available\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ No subsampled models trained. Please run the subsampled training cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34ee33",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps\n",
    "\n",
    "This notebook has provided a complete workflow for YOLO dataset preparation and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and recommendations\n",
    "print(\"ğŸ“‹ COMPREHENSIVE WORKFLOW SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Dataset summary\n",
    "if datasets:\n",
    "    print(\"âœ… Full Dataset Preparation:\")\n",
    "    for dataset_name, info in datasets.items():\n",
    "        print(f\"   â€¢ {dataset_name.title()} dataset: {info['train_images']}T + {info['val_images']}V images\")\n",
    "        print(f\"     Config: {info['yaml_file']}\")\n",
    "else:\n",
    "    print(\"âŒ No full datasets prepared\")\n",
    "\n",
    "# Subsampled dataset summary\n",
    "if subsampled_datasets:\n",
    "    total_subsamples = sum(len(subsamples) for subsamples in subsampled_datasets.values())\n",
    "    print(f\"\\nâœ… Subsampled Dataset Preparation:\")\n",
    "    print(f\"   â€¢ Created {total_subsamples} subsampled datasets for testing\")\n",
    "    for dataset_type, subsamples in subsampled_datasets.items():\n",
    "        sizes = list(subsamples.keys())\n",
    "        print(f\"   â€¢ {dataset_type.title()} subsamples: {sizes} images\")\n",
    "else:\n",
    "    print(\"\\nâŒ No subsampled datasets prepared\")\n",
    "\n",
    "# Full model training summary\n",
    "if trainer.training_results:\n",
    "    print(f\"\\nâœ… Full Model Training:\")\n",
    "    for model_name in trainer.training_results.keys():\n",
    "        result = trainer.training_results[model_name]\n",
    "        print(f\"   â€¢ {model_name} model trained\")\n",
    "        print(f\"     Best weights: {result['best_model_path']}\")\n",
    "        print(f\"     Results: {result['project_dir']}\")\n",
    "else:\n",
    "    print(\"\\nâŒ No full models trained\")\n",
    "\n",
    "# Subsampled model training summary\n",
    "if subsample_training_results:\n",
    "    total_subsample_models = sum(len(results) for results in subsample_training_results.values())\n",
    "    print(f\"\\nâœ… Subsampled Model Training:\")\n",
    "    print(f\"   â€¢ Trained {total_subsample_models} models on different dataset sizes\")\n",
    "    for dataset_type, results in subsample_training_results.items():\n",
    "        sizes = list(results.keys())\n",
    "        print(f\"   â€¢ {dataset_type.title()}: {sizes} image subsets\")\n",
    "else:\n",
    "    print(\"\\nâŒ No subsampled models trained\")\n",
    "\n",
    "# Next steps and recommendations\n",
    "print(f\"\\nğŸ” RECOMMENDED ANALYSIS:\")\n",
    "print(\"1. ğŸ“Š Compare full dataset models to determine: Original vs Background-Fused\")\n",
    "print(\"2. ğŸ“ˆ Analyze subsampled results to understand: Impact of dataset size\")\n",
    "print(\"3. ğŸ¯ Choose optimal approach based on:\")\n",
    "print(\"   - Performance requirements (mAP targets)\")\n",
    "print(\"   - Training time constraints\")\n",
    "print(\"   - Data availability\")\n",
    "\n",
    "print(f\"\\nğŸš€ DEPLOYMENT OPTIONS:\")\n",
    "if trainer.models:\n",
    "    print(\"Choose your best performing model for deployment:\")\n",
    "    for model_name in trainer.models.keys():\n",
    "        result = trainer.training_results[model_name]\n",
    "        print(f\"\\n   ğŸ“¦ {model_name} model:\")\n",
    "        print(\"   ```python\")\n",
    "        print(f\"   from ultralytics import YOLO\")\n",
    "        print(f\"   model = YOLO('{result['best_model_path']}')\")\n",
    "        print(\"   results = model.predict('your_image.jpg', conf=0.25)\")\n",
    "        print(\"   ```\")\n",
    "\n",
    "# Output files summary\n",
    "print(\"\\nğŸ“ Generated Files:\")\n",
    "if OUTPUT_BASE_DIR.exists():\n",
    "    print(f\"   â€¢ Datasets: {OUTPUT_BASE_DIR}\")\n",
    "    print(\"   â€¢ Training runs: runs/\")\n",
    "    if (OUTPUT_BASE_DIR / 'model_comparison.csv').exists():\n",
    "        print(f\"   â€¢ Comparison: {OUTPUT_BASE_DIR}/model_comparison.csv\")\n",
    "\n",
    "print(\"\\nğŸ‰ Workflow completed successfully!\")\n",
    "print(\"ğŸ’¡ You can now use your trained YOLO models for person detection.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_labeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
